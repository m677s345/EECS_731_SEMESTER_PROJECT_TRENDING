# EECS_731_SEMESTER_PROJECT_TRENDING
This repo serves as the hub for code from this project on analyzing trending YouTube videos. Regression, Clustering, Anomaly detection and Time series forecasting has been performed on the data.

Clustering (Madhumithra SK): Data cleaning was done prior to clustering which involved removing of columns including rating disabled, comments disabled, thumbnail_link, description, title, video_id. The clustering was performed on the data using DBScan and KMeans algorithm. The percentage like, dislike, comment were obtained using the values of likes, dislikes and comments and dividing them by view count. These along with the category ID were used to perform k-means clustering. The optimal value k=4 was obtained from elbow method and Silhoutte coefficient. Four clusters were found in the result. Tag count was obtained by counting the number of tags present in each row for each trending video. Also using the same percentage like, dislike and comment, and tag count instead of category ID, clustering using DB Scan was done. This also resulted in four clusters.

Regression (Emily Mikeska): Random Forest and Gradient Boosting regression models were trained and evaluated on the Time series data. As I discovered, Random Forest was not well suited for the problem of predicting popular categories of videos. Gradient Boosting regression performed better, but could still be improved. Both models predicted the most popular category ID based on the channel ID, view count, number of likes and dislikes, comment count, publish date, and trending date. To improve over the base model, I focused on optimizing the hyperparameters so that the models could be as accurate as possible with the data provided. The training data was spanned August 3, 2020 to mid October 15, 2020 and the test data spanned  from October 15, 2020 to October 27, 2020. 

Time Series Forecasting (Matt Stalcup): For time series forcasting modeling done on the YouTube trending dataset we first gathered information on tags and titles that could be useful in determining popularity. Using a word cloud to take the most frequently mentioned words in both the titles and tags of trending videos we were able to create a useful visualization of the distribution of tags over the timeframe we studied. We were able to pice together some relationships between tags like two part names and make and model numberd. We took the most popular tags and used them to interface with the google trends API. Google Trends was able to give us time series data for the tag outside of when it was trending. If we were to only use the data in the trending dataset the time series information would be binary in either is it trending or not. Using Trends we are able to see the relative popularity of the searched term over time. The trends data gives us the ability to train a time series forcasting model on the tags. The daily data that we got worked well for the ARMA model that we fit but not so well with the three fold cross validation model simply because there were very few datapoints. To see the model succede we would need to have a longer time frame with more datapoints. When compaired to the real search values for the forcasted popularity of a search term we see a good amount of disagreement in the general trend that is due to another spike in searches because of some additional content popularizing the search. Such annomalies are the target of the anomally detection portion of the project. However, we are able to successfully fit a model to the trends data for the youtube dataset and predict the relatve popularity of the tags.

Anomaly Detection (Adam Podgorny): Several approaches were made to attempt to separate anomalous behavior about a video that may indicate its likelyhood to trend. The baselines for this generally were established via PyTrends. First, highly frequently, middlingly frequent, and lowly frequent tags were separated, then assessed for trend separation. At the baseline, this did not appear to provide much in the way in discrimination power, and a more exhaustive dataset would have oversatured the PyTorch API, and as such, this approach was dropped. The next attempt was to determine by engagement metrics intracategory and extracategory exclusion metrics. That is, by treating the target category as an anomaly, and all exclusive of that as normal, any non trending videos should naturally fall under this as well. Solutions for this were attempted by Isolation Forest, Naive Bayes, and Neural Networks. None of these provided any useful discriminatary significantly above target category frequency. As such, methods only involved the raw data were discarded as well. The last approach attempted was to check for the relative search interest for the channel related to the videos in question. This required substantial subsetting to satisfy constraints on the PyTrends API. As such, only ~150 videos were selected. Interest at a date that was over some multiplier over the average and standard deviation for that channel was attempted. This yielded poor results as well. This implies that channel attention may not be an entirely appropriate proxy for video trending status, as there are clearly other correlatory factors involved confounding this. Perhaps with more data, a tractable and stable way of making these inferences could be made apparent, but this does not appear to be the case.
NB: the NoteBook1 is fairly perilous in terms of running it. It will try to run PyTrends in a fairly heavy way to preserve the first style of analysis. If the cells are run in a staggered fashion, this should work sufficiently. However running the whole notebook sequentially will very likely return a 429 error.
